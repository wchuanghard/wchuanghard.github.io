---
layout: post
title: "Implications of Deep Learning for Non-Human Systems"
date: 2016-06-27
---
William H. Chuang<br>
[v1] June 27, 2016 <br>

Scientists and engineers have long dreamed of building machines that think. This curiosity dates back to at least the time of Descartes or even earlier. Today, the machine that could replace humans have already on markets. Recently a company, called Foxconn, in China that produces all Apple’s physical products, including MacBook, iPhone, iPad, and so on, have successfully reduced its employee strength from 110,000 to 50,000(Anand). Today, we look to intelligent software and hardware that could cooperate with the software that runs on it to automate routine labor. In “What is a Mind?” this discipline is called connectionism. Today Connectionism (or Parallel Distributed Processing) is called Deep Learning, and Connectionism was the name of deep learning in 1980s to 1990s. In the 1940s to 1960s, Connectionism was called cybernetics, and in the meanwhile it also knows as artificial neural networks, or neural nets. The current renaming under the name deep learning began in 2006(Goodfellow et. al. 13). The main perspective on Deep Learning methods is that they are inspired by the biological brains.<br> 

Following, then, first of all, Deep Learning is inspired by the human brain and/or the brain of nonhuman animals. Further, we now have known that it works on a variety of applications. Thus, when we try to answer the question raised by the author of “What is a Mind?” that “If pattern recognition, rather than linguistically structured reasoning, turned out to be the most fundamental aspect of mental functioning, what implications of that have for the issue of nonhuman animals’ mental capabilities?”(220 Cunningham). We could see that from the Deep Learning method’s viewpoint, this could be a natural question, and from it's current success we know this question is not void, but a real question. Before answering the question, it’s better to remind ourselves that the author, Cunningham, also owns a point of view that “Perhaps no nonliving system will exactly model human mental functions”(219 Cunningham). This is Cunningham’s stand, but may not be agreed with today’s deep learning researchers. The reasons are in the following paragraphs. The original question is a combination of two subsections: one is in nonhuman living system, and the second is for nonliving system. Obviously, the second is a minimum boundary for the author that if we could show the nonliving system could have human mental functions, then to nonhuman animal the implications won’t be too hard to answer.<br>

Secondly, let’s focus on nonliving systems. “Is it possible in principle for a nonliving system like a computer or robot to be capable of some mental functions (processes, capacities)?”(219 Cunningham). From Deep Learning’s point of view, to answer the question we need to know a hierarchy structure/classification for animal’s brains. We all know that today humans have abilities to build a machine that implements the von Neumann architecture that developed by the mathematician and physicist John von Neumann and others in 1945 that is a computer architecture running programs on a central processing unit and storing data on a main mememory. Both CPU and the main memory are mianly built from metals and semiconductors which are made of atoms. On the other hand, all of the living things are made of atoms. Since in biology, insects are belong to animals, and if we take fruit flies as examples we could know that the fruit fly’s brain is made of proteins (they’re some meaningful combination of atoms), and we could also learn from biological science that these proteins are made according to fruit flies’ genes. Those molecules using the same physics to communicate each other as modern computers did that is they use electricity by sharing electrons (to form or disconnect chemical bonds), and not to mention for neural cells, also known as neurons, all the nerve messages are the use of electricity. The main point here is from the fundamental level, there is no much difference between them. So, why today we still didn’t see an AI that could think like humans? This could be answered by one notion that is mentioned in the “What is a Mind”: Mind is not a thing. 		<br>

In Connectionism, mind is a bunch of networks that is built on connections of neurons. Thus, in this perspective, how complicated the functions or process a mind could have is all depends on how complicated of its neuron network. Learning from biological brains, we now know that the existence of biological brains (and they’re successfully used in reality) could be seen as a proof that intelligent behavior is possible. Therefore a straightforward way to build intelligence is to do a reverse engineering on digging out the computational principles behind biological brains and duplicate their functionalities. By solving the engineering application problems, we could learn in an opposite way, and make a contribution on understanding of how biological brains work, what are the principles that are underlying human and nonhuman intelligence. What are those applications is not the focus of this paper, instead our focus is on the implications of these successes. Let’s step back to the connectionism’s focus: the number of connection per each neurons matters. For a fruit fly, each neuron has about 100 connections to other neurons, for mouse, each neuron has about 1000 connections to others; following, then, for humans the number of connections is about 10000(Goodfellow et. al. 21). Currently, we just reach the human level—with the same order of magnitude in the number of neural connections. However, this is not the whole story, because there are two other important factors: the number of neurons (not just the number of connections per a neuron), and the size of input data. The total number of neurons for an ant is about 100000, for a bee is about 1000000, and for a frog is about 10000000, then for humans the number is about 100000000000(Goodfellow et. al. 24). Our best Deep Learning models currently just barely reach the bee-level.<br>

The reason of why Deep Learning is useful is because, for instance, in speech recognition, previously, a project could cause a group of PhD student to write 3 months code and end up with the ability to machine to recognize a few people’s voices. On the contrary, Deep Learning uses a statistical method that takes the statistical result and by comparing the output of the neural nets to the correct label targets, it changes itself in each epoch in training. The result is in this way, people could write a few lines  of code (usually less than 50 lines of code), but end up with an ability to recognize a large group of people’s voice. The reason why this work and why we sometimes call Deep Learning as Deep Neural Nets is there are more than one hidden layer in the Deep Learning model. Before applying the model, no one knows how many hidden layers are there (that’s why the name was coined). After we feed in the data, if we found the accuracy is good enough, then we know that’s the right depth of layers, if the result is not good enough then we just add one more layer. Each layer, excluding the input layer, would have extracted essential features on each neuron, also known as patterns. In the more higher layer, the more abstract of the pattern would be learned. This process is parallel/analogue/similar to how people design a new word, coin a new term, or design a new symbol. One will try to find the spirit of the object or notion, and use that abstract spirit to design a new symbol that stands for the spirit. Now, the only remaining difference between the symbolic approach and connectionists’ approach is the paradox: correlation doesn’t mean causality. The definition of causality in statistical language would be described as we are 100% confident that a result must happen, and once the probability of the outcome deviates from 100%, it becomes a correlational/statistical/Connectionism approach; for example, in Deep Learning, we may say the prediction has 3% error rate. However, do we really have 100% accuracy in a causality approach (that is the symbolic approach), or we just make a lot of constraints and assumptions to exclude out all the other cases that we don’t want to see, and in a statistical approach (that is the Connectionists/Deep Learning approach) we just don’t do the exclusion? Also, in a temporal order of the input data, the Deep Learning model is able to distinguish what is the cause, and what is the result. Thus, based on this understanding we could see the implication of the question, and furthermore, we see a merge between the two approaches: We start from connectionism, and could end up with symbolism. In other words, we derive the symbols within each neuron, and each symbol is an abstract pattern that capture by the Deep Learning method.<br>

Finally, if we combine the previous paragraph and the notions: the number of neurons and connections really matters, we can end up with an understanding: At a fundamental level, according to Deep Learning’s discovery, there is no difference between human animal and nonhuman animals.  They share the same mechanism that is used by the Deep Learning methods. What made the current difference between human systems and nonhuman system is due to the number of connections of neurons, and the number of neurons. Then, one implication is, if tomorrow we edit a chimp gene that are controlling these factors of chimp’s neural nets in its brain, then it’s very possible we will have a chimp that could have human linguistic ability. The other implication might be, to a lot of us,  our world view might become different, once we learn this. Also, a further implication might be once we edit plant’s gene, then one day we might be able to talk to a tree, a flower, or a bird.<br>

<p style="text-align: center">Works Cited</p><br>
Cunningham, S. “What is a Mind?” Hackett Publishing Company, Inc. 2000.<br>
Anand, Kunal. "3000 Engineers Freed Up At Wipro After Artificial Intelligence Learns To Do Their Work!" Indiatimes.com. Indiatimes.com, 9 June 2016. Web. 27 June 2016. <http://www.indiatimes.com/news/world/3000-software-engineers-might-get-fired-at-wipro-after-artificial-intelligence-learns-to-do-all-their-jobs-256519.html>.<br>
