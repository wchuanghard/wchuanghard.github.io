---
layout: default
title: "William Chuang"
description: "Graduate Student at University of Arizona"
---

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>William Chuang</title>
  <!-- Link to your CSS file -->
  <link rel="stylesheet" href="/css/main.css">
</head>
<body>

  <!-- Top Navigation Bar -->
  <header>
    <nav class="top-nav">
      <ul>
        <!-- Single-line top menu -->
        <li><a href="/">Home</a></li>
        <!-- <li><a href="/about">About</a></li> -->
        <li><a href="/files/william_chuang_cv.pdf">CV</a></li>
        <li><a href="/files/William_advanced_mathematics_courses.pdf">Math Courses Taken</a></li>
        <!-- <li><a href="/projects">Projects</a></li> -->
        <!-- <li><a href="/CS_Notes">CS Notes</a></li> -->
        <!-- <li><a href="/files/Research_Statement.pdf">Research Statement</a></li> -->
        <li><a href="/Research">Research</a></li>
        <!-- <li><a href="/blog">Teaching Samples</a></li> -->
        <li><a href="/Teaching">Teaching</a></li>
        <li><a href="/My Schedule">My Schedule</a></li>
        <!-- <li><a href="/files/William_H_Chuang_personal_history.pdf">My Math Journey</a></li> -->
      </ul>
    </nav>
  </header>

  <!-- Main Container -->
  <main class="container">
    
    <!-- Intro / Blurb Section -->
    <section class="blurb">
      <h1 class="title">William Chuang</h1>
      <p class="subtitle">Graduate Student, University of Arizona</p>

      <div class="profile-image">
        <!-- If you want the image at 90% width, do inline or via CSS -->
        <img 
          src="/files/me_at_Notre_Dame.jpg"
          alt="Cathédrale Notre-Dame de Paris—my faith!"
          style="width: 90%;"
        />
      </div>

      <p class="contact-info">
        <strong>E-mail:</strong> williamchuang@arizona.edu
      </p>
      <p class="contact-info">
        <strong>Pronouns:</strong> he/him
      </p>

      <hr class="divider" />

      <p class="content-text">
        I enjoy hiking and painting (including oil painting), and I am also deeply
        curious about neuroscience, psychology, and classical studies. I am 
        particularly fascinated by innovative strategies for designing and 
        coding neural network models—especially those that can recursively 
        design, implement, and train new, evolved networks based on a parent 
        model’s best configuration. In addition, I love exploring how, when, 
        and what we learn to accomplish a chosen mission along all possible 
        paths, ultimately working toward a comprehensive “user manual” for 
        the human mind.
      </p>

      <p class="content-text">
        I have curated a selection of notes and resources to support 
        preparation for qualifying exams. These materials reflect some of my 
        approaches to key topics and problem-solving strategies. They are 
        available for review in the following Google Drive folder:
        <br>
        <a
          href="https://drive.google.com/drive/folders/1HDanu8evZ7ytPe4lZSUTETJmK4WZ2R2T?usp=sharing"
          class="external-link"
        >Access my Qualifying Exam Notes</a>
      </p>

      <hr class="divider" />

      <p class="content-text">
        Additionally, here is my YouTube channel, where I plan to share worked-through math problems regularly:
        <a 
          href="https://www.youtube.com/channel/UCdN4ayC6Q53Zs0A4_aYlBAA"
          class="external-link"
        >@william_chuang</a>
      </p>

      <hr class="divider" />

      <p class="content-text">
        You can find some of my older math notes here:
        <br>
        <a
          href="https://drive.google.com/drive/folders/1u2csqSsWxGHkgSIGil3xRVwxsYxwY2jI?usp=share_link"
          class="external-link"
        >My old notes</a>
      </p>

      <hr class="divider" />

      <p class="content-text">
        <strong>More About Me Before 2015</strong>
        <br>
        <a 
          href="/files/till_2014.pdf"
          class="external-link"
        >Detailed Records Prior to 2014</a>
      </p>

      <hr class="divider" />

      <p class="content-text">
        <strong>The Genealogy of the Chuang Family (also spelled Cheung, Chong, etc.)</strong>
      </p>
      <p class="content-text">
        According to statistics from the U.S. Census Bureau, there are an 
        estimated 2,931 individuals in the United States with the surname Chuang, 
        ranking it 11,621st in prevalence, at 0.92 per 100,000 people. In East 
        Asia, the name Chuang remains relatively rare; it ranks 323rd in the Song 
        Dynasty’s “Hundred Surnames.” As a result, many individuals with the 
        surname Chuang are unfamiliar with their lineage beyond three generations 
        and may even feel like outsiders to the larger clan.
      </p>

      <p class="content-text">
        Historical data indicates that fewer than 50,000 people (around 0.06% of 
        the total population) bore the surname Chuang during the Song Dynasty, 
        with Fujian hosting the highest concentration. By the Ming Dynasty, 
        approximately 120,000 people (about 0.12% of the total population) 
        carried this surname. Today, the name is most commonly found in Guangdong, 
        Fujian, Taiwan, Jiangsu, Zhejiang, Shandong, Heilongjiang, Jilin, 
        Shanghai, and Liaoning.
      </p>

      <p class="content-text">
        My mother was born in Wucuo (now Erlun) in Yunlin County, and my father 
        was born in Erlin Township—once known as Gielem (a region known for deer) 
        to the Dutch. I was born in New Taipei City, Taiwan, in 1988. Based on my 
        grandparents’ family records, I am the 20th generation of the Chuang 
        family to settle in Taiwan after our ancestors first arrived in the 1600s, 
        following the Dutch occupation. This migration reminds me of the Mayflower 
        pilgrims who arrived in the Americas around the same time—my ancestors 
        similarly ventured forth in pursuit of freedom and opportunity.
      </p>

      <p class="content-text">
        A more recent notable figure in the Chuang family is Zhuang Yunkuan (also 
        known as Yunkuan Chuang), who served as both a Qing Dynasty and Republic 
        of China politician, as well as a Chinese calligrapher. He was a delegate 
        in drafting the Republic of China’s provisional constitution and, in 1925, 
        joined the board of directors of the National Palace Museum.
      </p>

      <p class="content-text">
        There is also a branch of the Chuang family in Guangdong and Hong Kong. 
        Among its most well-known members are Cheung Jing-on, his daughter 
        Chong Yuet-ming, and his nephew.
      </p>
    </section>

    <!-- Inserted Content: ALL Centered -->
    <section class="research-journey centered-content">
      <h2>Advancing Transformer Efficiency Through Dynamic Scaling Factors: My Research Journey</h2>

      <h3>Introduction</h3>
      <p>
        The transformer architecture has revolutionized deep learning, powering state-of-the-art large language models (LLMs) such as GPT-4. However, the reliance on brute computational power to scale these models presents significant challenges, including high costs and inefficiency. My research focuses on dynamically optimizing the scaling factor \(\beta\) in transformers to improve efficiency and accuracy. This journey has been both challenging and rewarding, and I am proud to share the progress I have made.
      </p>
      <hr class="divider">

      <h3>Timeline and Research Progress</h3>
      <div class="timeline">
        <div class="timeline-item">
          <h4>December 2022 – January 2023</h4>
          <ul>
            <li>Began investigating the role of the scaling factor \(\beta\) in self-attention mechanisms.</li>
            <li>Developed theoretical foundations inspired by statistical mechanics and optimization theory to dynamically adjust \(\beta\).</li>
          </ul>
        </div>

        <div class="timeline-item">
          <h4>December 2023</h4>
          <ul>
            <li><strong>RTG Presentation</strong>: Presented a preliminary version of my work at the RTG seminar at the University of Arizona.
              <ul>
                <li>The presentation focused on moderate improvements in model performance by dynamically optimizing \(\beta\).</li>
                <li>Received mixed feedback, with some skepticism due to the lack of large-scale demonstrations.</li>
              </ul>
            </li>
          </ul>
        </div>

        <div class="timeline-item">
          <h4>September 2023</h4>
          <ul>
            <li>Drafted the first version of my research paper, focusing on the theoretical basis and moderate empirical results to maintain credibility while avoiding overstatements.</li>
          </ul>
        </div>

        <div class="timeline-item">
          <h4>October 30, 2024</h4>
          <ul>
            <li><strong>Export Office Rejection</strong>:
              <ul>
                <li>Contacted the Export Control Office at the University of Arizona to ensure compliance with dual-use regulations.</li>
                <li>Despite explaining the potential dual-use nature of my work, the export office declined to classify it as significant or requiring clearance.</li>
                <li><strong>Their Response</strong>: "We do not need to clear your work on any of the projects you have described."</li>
                <li><strong>Impact</strong>: This rejection reflected a lack of institutional recognition of the potential importance of my work for U.S. competitiveness and national security.</li>
              </ul>
            </li>
          </ul>
        </div>

        <div class="timeline-item">
          <h4>December 2024</h4>
          <ul>
            <li>Published the work on <strong>ResearchGate</strong> to ensure accessibility and transparency. While ResearchGate has a smaller reach than arXiv, it allowed me to share my results with the academic community.</li>
          </ul>
        </div>

        <div class="timeline-item">
          <h4>January 2025</h4>
          <ul>
            <li>Preparing further refinements to the paper, incorporating additional experimental results and practical implications to submit to alternative venues.</li>
          </ul>
        </div>
      </div>
      <hr class="divider">

      <h3>Key Contributions</h3>
      <ol>
        <li>
          <strong>Dynamic Scaling Factor Optimization</strong>:
          <ul>
            <li>Proposed a dynamic adjustment to the traditional scaling factor (\(\beta = \frac{1}{\sqrt{d_k}}\)) used in transformers.</li>
            <li>Demonstrated that a dynamically optimized \(\beta\) significantly improves test accuracy across various datasets and model configurations.</li>
            <li>Published moderate results showing substantial improvements over traditional methods without overstating claims.</li>
          </ul>
        </li>
        <li>
          <strong>Experimental Results</strong>:
          <ul>
            <li>The results showcase consistent improvements in accuracy when using the dynamic scaling factor compared to the traditional fixed method.</li>
            <li>Key findings include accuracy improvements across varying categories, sequence lengths, and training set sizes.</li>
          </ul>
        </li>
        <li>
          <strong>Theoretical Foundation</strong>:
          <ul>
            <li>Derived the dynamic scaling factor optimization method based on insights from statistical mechanics and energy minimization principles.</li>
            <li>Demonstrated the theoretical soundness of the method in reducing redundancy and enhancing efficiency in self-attention mechanisms.</li>
          </ul>
        </li>
      </ol>
      <hr class="divider">

      <h3>How My Research Could Have Prevented the $2 Trillion Market Evaporation</h3>
      <p><strong>The Problem:</strong></p>
      <p>
        On January 27, 2025, U.S. markets lost $2 trillion in value due to the emergence of cost-efficient AI models like DeepSeek R1, developed by a Chinese AI startup. DeepSeek claimed to have built a large language model rivaling GPT-4 at a fraction of the cost, leveraging extreme efficiency and open-source access. This disruption highlighted vulnerabilities in the U.S. AI ecosystem, which relies heavily on brute computational power and proprietary, high-cost approaches.
      </p>

      <p><strong>How My Work Could Have Helped:</strong></p>
      <ol>
        <li>
          <strong>Cost-Efficiency Leadership:</strong>
          <ul>
            <li>My research on dynamically optimizing the scaling factor \(\beta\) addresses the inefficiencies that made U.S. companies vulnerable to DeepSeek's disruption.</li>
            <li>By significantly improving transformer efficiency, my approach could have reduced the training costs of models like GPT-4, allowing U.S. companies to compete on cost with DeepSeek.</li>
          </ul>
        </li>
        <li>
          <strong>Early Adoption by U.S. Companies:</strong>
          <ul>
            <li>If my work had been published on arXiv earlier and adopted by companies like NVIDIA or OpenAI, they could have implemented cost-saving measures to preempt DeepSeek's advantage.</li>
            <li>This would have demonstrated U.S. leadership in AI efficiency, reducing the market shock caused by DeepSeek's announcement.</li>
          </ul>
        </li>
        <li>
          <strong>Strengthening U.S. AI Leadership:</strong>
          <ul>
            <li>My work offers a paradigm shift from brute computational scaling to intelligent optimization, positioning U.S. companies to lead the AI industry in both performance and cost-efficiency.</li>
            <li>Early integration of my methods into U.S. models would have preserved investor confidence in U.S. tech companies, preventing the massive market correction.</li>
          </ul>
        </li>
        <li>
          <strong>National and Economic Security:</strong>
          <ul>
            <li>My results could have been framed as part of a broader strategy to secure U.S. leadership in AI, reducing the risk of economic shocks caused by foreign competition.</li>
            <li>By highlighting the strategic importance of efficiency-focused AI research, my work could have attracted government and industry support, ensuring its timely implementation.</li>
          </ul>
        </li>
      </ol>
      <hr class="divider">

      <h3>Challenges Faced</h3>
      <ol>
        <li>
          <strong>Skepticism and Rejections:</strong>
          <ul>
            <li>Initial skepticism from academic peers due to the bold nature of the proposed changes to a widely used formula in transformers.</li>
            <li>Multiple rejections from arXiv, citing insufficient originality and substantive contribution, possibly influenced by limited visibility within the academic community.</li>
          </ul>
        </li>
        <li>
          <strong>Institutional Hurdles:</strong>
          <ul>
            <li>The Export Control Office at the University of Arizona, despite recognizing the dual-use potential of my research, declined to classify it as requiring special clearance.</li>
            <li>This lack of institutional recognition limited the visibility and perceived importance of my work.</li>
          </ul>
        </li>
        <li>
          <strong>Resource Limitations:</strong>
          <ul>
            <li>Without access to large-scale computational resources, I was unable to demonstrate the results on models comparable to GPT-4, which may have contributed to skepticism.</li>
          </ul>
        </li>
      </ol>
      <hr class="divider">

      <h3>Published Results</h3>
      <p>
        The key results from my work are summarized in the following table:
      </p>

      <div class="table-responsive">
        <table class="results-table">
          <thead>
            <tr>
              <th>Scaling Factor</th>
              <th>Accuracy (%)</th>
              <th>Observations</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>\(\beta = \frac{1}{\sqrt{d_k}}\) <br/> ~0.25</td>
              <td>1.50</td>
              <td>Traditional method</td>
            </tr>
            <tr>
              <td>\(\beta_{\text{opt}} = 6.67\)</td>
              <td>96.48</td>
              <td>Dynamic method</td>
            </tr>
          </tbody>
        </table>
      </div>
      <hr class="divider" />
    </section>
    <!-- Inserted Content Ends Here -->
    
  </main>

  <!-- Footer -->
  <footer>
    <ul class="footer-links">
      <!-- <li><a href="mailto:whchuang@usfca.edu">email</a></li> -->
      <li>
        <a href="https://github.com/williamchuang"
          >https://github.com/williamchuang</a
        >
      </li>
    </ul>
  </footer>

</body>
</html>

