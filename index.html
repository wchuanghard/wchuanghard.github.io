---
layout: default
title: "William Chuang"
description: "Graduate Student at University of Arizona"
---

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>William Chuang</title>
 
  <link rel="stylesheet" href="/css/main.css">
</head>
<body>


  <!-- Main Container -->
  <main class="container">
    
  
    <section class="blurb">
      <h1 class="title">William Chuang</h1>
      <p class="subtitle">Graduate Student, University of Arizona</p>

      <div class="profile-image">
        <!-- If you want the image at 90% width, do inline or via CSS -->
        <img 
          src="/files/me_at_Notre_Dame.jpg"
          alt="Cathédrale Notre-Dame de Paris—my faith!"
          style="width: 90%;"
        />
      </div>

      <p class="contact-info">
        <strong>E-mail:</strong> williamchuang@arizona.edu
      </p>
      <p class="contact-info">
        <strong>Pronouns:</strong> he/him
      </p>

      <hr class="divider" />

      <p class="content-text">
        I enjoy hiking and painting (including oil painting), and I am also deeply
        curious about neuroscience, psychology, and classical studies. I am 
        particularly fascinated by innovative strategies for designing and 
        coding neural network models—especially those that can recursively 
        design, implement, and train new, evolved networks based on a parent 
        model’s best configuration. In addition, I love exploring how, when, 
        and what we learn to accomplish a chosen mission along all possible 
        paths, ultimately working toward a comprehensive “user manual” for 
        the human mind.
      </p>

      <p class="content-text">
        I have curated a selection of notes and resources to support 
        preparation for qualifying exams. These materials reflect some of my 
        approaches to key topics and problem-solving strategies. They are 
        available for review in the following Google Drive folder:
        <br>
        <a
          href="https://drive.google.com/drive/folders/1HDanu8evZ7ytPe4lZSUTETJmK4WZ2R2T?usp=sharing"
          class="external-link"
        >Access my Qualifying Exam Notes</a>
      </p>

      <hr class="divider" />

      <p class="content-text">
        Additionally, here is my YouTube channel, where I plan to share worked-through math problems regularly:
        <a 
          href="https://www.youtube.com/channel/UCdN4ayC6Q53Zs0A4_aYlBAA"
          class="external-link"
        >@william_chuang</a>
      </p>

      <hr class="divider" />

      <p class="content-text">
        You can find some of my older math notes here:
        <br>
        <a
          href="https://drive.google.com/drive/folders/1u2csqSsWxGHkgSIGil3xRVwxsYxwY2jI?usp=share_link"
          class="external-link"
        >My old notes</a>
      </p>

      <hr class="divider" />

      <p class="content-text">
        <strong>More About Me Before 2015</strong>
        <br>
        <a 
          href="/files/till_2014.pdf"
          class="external-link"
        >Detailed Records Prior to 2014</a>
      </p>

      <hr class="divider" />

      <p class="content-text">
        <strong>The Genealogy of the Chuang Family (also spelled Cheung, Chong, etc.)</strong>
      </p>
      <p class="content-text">
        According to statistics from the U.S. Census Bureau, there are an 
        estimated 2,931 individuals in the United States with the surname Chuang, 
        ranking it 11,621st in prevalence, at 0.92 per 100,000 people. In East 
        Asia, the name Chuang remains relatively rare; it ranks 323rd in the Song 
        Dynasty’s “Hundred Surnames.” As a result, many individuals with the 
        surname Chuang are unfamiliar with their lineage beyond three generations 
        and may even feel like outsiders to the larger clan.
      </p>

      <p class="content-text">
        Historical data indicates that fewer than 50,000 people (around 0.06% of 
        the total population) bore the surname Chuang during the Song Dynasty, 
        with Fujian hosting the highest concentration. By the Ming Dynasty, 
        approximately 120,000 people (about 0.12% of the total population) 
        carried this surname. Today, the name is most commonly found in Guangdong, 
        Fujian, Taiwan, Jiangsu, Zhejiang, Shandong, Heilongjiang, Jilin, 
        Shanghai, and Liaoning.
      </p>

      <p class="content-text">
        My mother was born in Wucuo (now Erlun) in Yunlin County, and my father 
        was born in Erlin Township—once known as Gielem (a region known for deer) 
        to the Dutch. I was born in New Taipei City, Taiwan, in 1988. Based on my 
        grandparents’ family records, I am the 20th generation of the Chuang 
        family to settle in Taiwan after our ancestors first arrived in the 1600s, 
        following the Dutch occupation. This migration reminds me of the Mayflower 
        pilgrims who arrived in the Americas around the same time—my ancestors 
        similarly ventured forth in pursuit of freedom and opportunity.
      </p>

      <p class="content-text">
        A more recent notable figure in the Chuang family is Zhuang Yunkuan (also 
        known as Yunkuan Chuang), who served as both a Qing Dynasty and Republic 
        of China politician, as well as a Chinese calligrapher. He was a delegate 
        in drafting the Republic of China’s provisional constitution and, in 1925, 
        joined the board of directors of the National Palace Museum.
      </p>

      <p class="content-text">
        There is also a branch of the Chuang family in Guangdong and Hong Kong. 
        Among its most well-known members are Cheung Jing-on, his daughter 
        Chong Yuet-ming, and his nephew.
      </p>
    </section>

    <!-- Inserted Content: ALL Centered -->
    <section class="research-journey centered-content">
      <h2>Advancing Transformer Efficiency Through Dynamic Scaling Factors: My Research Journey</h2>

      <h3>Introduction</h3>
      <p>
        The transformer architecture has revolutionized deep learning, powering state-of-the-art large language models (LLMs) such as GPT-4. However, the reliance on brute computational power to scale these models presents significant challenges, including high costs and inefficiency. My research focuses on dynamically optimizing the scaling factor \(\beta\) in transformers to improve efficiency and accuracy. This journey has been both challenging and rewarding, and I am proud to share the progress I have made.
      </p>
      <hr class="divider">

      <h3>Timeline and Research Progress</h3>
      <div class="timeline">
        <div class="timeline-item">
          <h4>December 2022 – January 2023</h4>
          <ul>
            <li>Began investigating the role of the scaling factor \(\beta\) in self-attention mechanisms.</li>
            <li>Developed theoretical foundations inspired by statistical mechanics and optimization theory to dynamically adjust \(\beta\).</li>
          </ul>
        </div>

        <div class="timeline-item">
          <h4>September 2023</h4>
          <ul>
            <li>Drafted the first version of my research paper, focusing on the theoretical basis and moderate empirical results to maintain credibility while avoiding overstatements.</li>
          </ul>
        </div>

        <div class="timeline-item">

        <div class="timeline-item">
          <h4>December 2023</h4>
          <ul>
            <li><strong>RTG Presentation</strong>: Presented a preliminary version of my work at the RTG seminar at the University of Arizona.
              <ul>
                <li>The presentation focused on moderate improvements in model performance by dynamically optimizing \(\beta\).</li>
                <li>Received mixed feedback, with some skepticism due to the lack of large-scale demonstrations.</li>
              </ul>
            </li>
          </ul>
        </div>

<div class="timeline-item">
  <h4>October 30, 2024</h4>
  <ul>
    <li><strong>Export Office Rejection</strong>:
      <ul>
        <li>Contacted the Export Control Office at the University of Arizona to ensure compliance with dual-use regulations.</li>
        <li>Despite explaining the potential dual-use nature of my work, the export office declined to classify it as significant or requiring clearance.</li>
        <li><strong>Their Response</strong>: "We do not need to clear your work on any of the projects you have described."</li>
        <li><strong>Impact</strong>: This rejection reflected a lack of institutional recognition of the potential importance of my work for U.S. competitiveness and national security.</li>
        <!-- Images Section -->
        <li class="images-section">
          <div class="image-container">
            <figure>
              <img src="/files/export_office.png" alt="Description of Transformer-Based LLM Training Efficiency" class="timeline-image">
              <figcaption>Portion of the description I wrote.</figcaption>
            </figure>
          </div>
          <div class="image-container">
            <figure>
              <img src="/files/export_office-reply.png" alt="Export Office Reply" class="timeline-image">
              <figcaption>Last email I received from the Export Control Office.</figcaption>
            </figure>
          </div>
        </li>
      </ul>
    </li>
  </ul>
</div>

        <div class="timeline-item">
          <h4>December 2024</h4>
          <ul>
            <li>Published the work on <strong>ResearchGate</strong> to ensure accessibility and transparency. While ResearchGate has a smaller reach than arXiv, it allowed me to share my results with the academic community.</li>
          </ul>
        </div>

        <div class="timeline-item">
          <h4>January 2025</h4>
          <ul>
            <li>Preparing further refinements to the paper, incorporating additional experimental results and practical implications to submit to alternative venues.</li>
          </ul>
        </div>
      </div>
      <hr class="divider">

      <h3>Key Contributions</h3>
      <ol>
        <li>
          <strong>Dynamic Scaling Factor Optimization</strong>:
          <ul>
            <li>Proposed a dynamic adjustment to the traditional scaling factor (\(\beta = \frac{1}{\sqrt{d_k}}\)) used in transformers.</li>
            <li>Demonstrated that a dynamically optimized \(\beta\) significantly improves test accuracy across various datasets and model configurations.</li>
            <li>Published moderate results showing substantial improvements over traditional methods without overstating claims.</li>
          </ul>
        </li>
        <li>
          <strong>Experimental Results</strong>:
          <ul>
            <li>The results showcase consistent improvements in accuracy when using the dynamic scaling factor compared to the traditional fixed method.</li>
            <li>Key findings include accuracy improvements across varying categories, sequence lengths, and training set sizes.</li>
          </ul>
        </li>
        <li>
          <strong>Theoretical Foundation</strong>:
          <ul>
            <li>Derived the dynamic scaling factor optimization method based on insights from statistical mechanics and energy minimization principles.</li>
            <li>Demonstrated the theoretical soundness of the method in reducing redundancy and enhancing efficiency in self-attention mechanisms.</li>
          </ul>
        </li>
      </ol>
      <hr class="divider">

      <h3>How My Research Could Have Prevented the $2 Trillion Market Evaporation</h3>
      <p><strong>The Problem:</strong></p>
      <p>
        On January 27, 2025, U.S. markets lost $2 trillion in value due to the emergence of cost-efficient AI models like DeepSeek R1, developed by a Chinese AI startup. DeepSeek claimed to have built a large language model rivaling GPT-4 at a fraction of the cost, leveraging extreme efficiency and open-source access. This disruption highlighted vulnerabilities in the U.S. AI ecosystem, which relies heavily on brute computational power and proprietary, high-cost approaches.
      </p>

      <p><strong>How My Work Could Have Helped:</strong></p>
      <ol>
        <li>
          <strong>Cost-Efficiency Leadership:</strong>
          <ul>
            <li>My research on dynamically optimizing the scaling factor \(\beta\) addresses the inefficiencies that made U.S. companies vulnerable to DeepSeek's disruption.</li>
            <li>By significantly improving transformer efficiency, my approach could have reduced the training costs of models like GPT-4, allowing U.S. companies to compete on cost with DeepSeek.</li>
          </ul>
        </li>
        <li>
          <strong>Early Adoption by U.S. Companies:</strong>
          <ul>
            <li>If my work had been published on arXiv earlier and adopted by companies like NVIDIA or OpenAI, they could have implemented cost-saving measures to preempt DeepSeek's advantage.</li>
            <li>This would have demonstrated U.S. leadership in AI efficiency, reducing the market shock caused by DeepSeek's announcement.</li>
          </ul>
        </li>
        <li>
          <strong>Strengthening U.S. AI Leadership:</strong>
          <ul>
            <li>My work offers a paradigm shift from brute computational scaling to intelligent optimization, positioning U.S. companies to lead the AI industry in both performance and cost-efficiency.</li>
            <li>Early integration of my methods into U.S. models would have preserved investor confidence in U.S. tech companies, preventing the massive market correction.</li>
          </ul>
        </li>
        <li>
          <strong>National and Economic Security:</strong>
          <ul>
            <li>My results could have been framed as part of a broader strategy to secure U.S. leadership in AI, reducing the risk of economic shocks caused by foreign competition.</li>
            <li>By highlighting the strategic importance of efficiency-focused AI research, my work could have attracted government and industry support, ensuring its timely implementation.</li>
          </ul>
        </li>
      </ol>
      <hr class="divider">

      <h3>Published Results</h3>
      <p>
        The key results from my work are summarized in the following table:
      </p>

      <div class="table-responsive">
        <table class="results-table">
          <thead>
            <tr>
              <th>Scaling Factor</th>
              <th>Accuracy (%)</th>
              <th>Observations</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>\(\beta = \frac{1}{\sqrt{d_k}}\) <br/> ~0.25</td>
              <td>1.50</td>
              <td>Traditional method</td>
            </tr>
            <tr>
              <td>\(\beta_{\text{opt}} = 6.67\)</td>
              <td>96.48</td>
              <td>Dynamic method</td>
            </tr>
          </tbody>
        </table>
      </div>
      <hr class="divider" />
    </section>
<h3>Future Directions</h3>
<p>
    Future work should extend this approach to diverse, real-world tasks, such as language modeling, machine translation, and computer vision. Second, the n-nary search algorithm, though effective, introduces computational overhead, which may be prohibitive for large-scale models. Developing more efficient algorithms for dynamically estimating \(\beta_{\text{opt}}\) is an important area for future research. Lastly, the interaction between \(\beta\) and other critical hyperparameters, such as learning rate, weight initialization, and attention head configurations, remains underexplored. A more comprehensive understanding of these interactions could lead to more holistic approaches to hyperparameter optimization.
</p>

<p>
    My research from September 2023 demonstrated the necessity of examining all other hyperparameters to uncover potential redundancies in transformer architectures. By employing concepts from the Ising model (spin-lattice, i.e., quantized), it becomes possible to compress or distill models further by quantizing parameter values. Parameters that are zeroed out through this approach reveal redundancy within the model. This insight suggests that the quantization process, informed by spin-lattice physics, could be a key tool for improving model efficiency and scalability.
</p>

<p>
    Furthermore, the algorithm I proposed for dynamically estimating \(\beta_{\text{opt}}\) can be translated into a reinforcement training framework. In this context, the algorithm adapts \(\beta\) in response to model feedback, effectively treating the optimization of \(\beta\) as a reinforcement learning problem. By iteratively refining \(\beta_{\text{opt}}\) based on real-time performance metrics, the algorithm could enable more adaptive, task-specific training. This approach not only enhances model efficiency but also provides a pathway for integrating reinforcement training principles into hyperparameter optimization.
</p>

<section class="research-implications">
  <h2>Theoretical and Strategic Implications of My Research</h2>

  <h3>1. Challenging the Mainstream Perspective: "Attention is All You Need"</h3>
  <p>
    The widespread adoption of the scaling factor \( \beta = \frac{1}{\sqrt{d_k}} \) in transformer-based architectures, as introduced by Vaswani et al. (2017) in "Attention is All You Need," lacks rigorous theoretical justification in modern large-scale implementations. This heuristic, though convenient, assumes that the variance of the attention score matrix remains constant during training—a simplification that breaks down as models scale and undergo dynamic updates. 
  </p>
  <p>
    Insisting on this traditional approach without revisiting its mathematical foundation essentially encourages researchers and industry leaders to take <strong>shortcuts</strong>—not in the sense of reducing training cost or improving theoretical development, but in <strong>prioritizing rapid implementation and commercialization over fundamental scientific rigor</strong>. By rushing to deploy models and bring AI products to market, mainstream researchers neglect deeper structural insights, leading to suboptimal architectures that persist due to inertia in the field.
  </p>
  <p>
    The implications are profound: adhering to outdated methods risks stagnation in the field, perpetuating inefficiencies and hindering progress. My research not only addresses this gap but also paves the way for a principled re-evaluation of foundational assumptions in transformer architectures.
  </p>

  <h3>2. Strategic Implications of DeepSeek's Efficiency</h3>
  <p>
    The emergence of cost-efficient AI models, such as DeepSeek R1 developed by a Chinese startup, underscores the strategic vulnerabilities inherent in the U.S. AI ecosystem. By leveraging extreme efficiency, DeepSeek has demonstrated the capability to rival state-of-the-art models like GPT-4 at a fraction of the cost. This development is not merely a technological achievement but a potential game-changer in geopolitics and economic security.
  </p>
  <p>
    If the PRC employs such advancements to iteratively improve their AI models and subsequently use those models to optimize semiconductor design and manufacturing, the implications are alarming. The current restrictions on NVIDIA, TSMC, and other chip manufacturers would become significantly less effective. Enhanced AI models could drive innovations in chip architecture, fabrication techniques, and supply chain efficiency, enabling the PRC to circumvent traditional dependencies on U.S. and allied technologies.
  </p>
  <p>
    My research offers a counter-strategy by emphasizing efficiency and cost reduction in transformer architectures. By adopting dynamic scaling methods, U.S. companies can maintain a competitive edge, mitigating the risk of market disruptions caused by foreign competitors. Furthermore, integrating my methods into U.S. AI and semiconductor strategies would reinforce national security, ensuring leadership in critical technologies.
  </p>

  <h3>3. Strategic Countermeasures: Ensuring U.S. AI and Semiconductor Superiority</h3>
  <p>
    To counter the potential developments enabled by PRC’s AI-driven semiconductor advancements, proactive solutions must be implemented. Below are two key strategies:
  </p>

  <h4>Solution 1: Establishing AI-Empowered Semiconductor R&D Initiatives</h4>
  <p>
    The U.S. must invest heavily in AI-accelerated semiconductor research to preemptively outpace adversarial developments. This includes:
    <ul>
      <li>Integrating AI-driven chip design optimization using dynamic scaling factors to enhance efficiency and reduce costs.</li>
      <li>Funding collaborative research efforts between national labs, defense contractors, and academic institutions to develop secure, energy-efficient AI chips.</li>
      <li>Leveraging my research on transformer scaling factors to fine-tune AI-based chip architecture for superior performance.</li>
    </ul>
  </p>

  <h4>Solution 2: AI-Optimized Export Controls and Supply Chain Resilience</h4>
  <p>
    Traditional export control mechanisms are becoming less effective in a world where AI can independently design optimized chips. Instead, the U.S. must:
    <ul>
      <li>Implement AI-driven monitoring systems to track semiconductor supply chains and detect unauthorized technology transfers.</li>
      <li>Develop reinforcement-learning-based dynamic trade policies that adapt in real-time to counteract PRC's AI-driven semiconductor progress.</li>
      <li>Ensure that any AI model used in high-security domains incorporates dynamic security protocols to prevent adversarial exploitation.</li>
    </ul>
  </p>

  <p>
    These countermeasures are essential to ensuring that U.S. advancements in AI and semiconductor technologies remain strategically dominant. By integrating adaptive scaling techniques into AI-driven semiconductor R&D and export control strategies, the U.S. can effectively neutralize emerging threats and maintain technological superiority.
  </p>
</section>

  </main>

  <!-- Footer -->
  <footer>
    <ul class="footer-links">
      <!-- <li><a href="mailto:whchuang@usfca.edu">email</a></li> -->
      <li>
        <a href="https://github.com/williamchuang"
          >https://github.com/williamchuang</a
        >
      </li>
    </ul>
  </footer>

</body>
</html>

